# EmpactVR-MLP

EMPACTVR is a research project to integrate bi-modal (facial and audio) emotion recognition, into a metaverse or game platform. This is the first version of EMPACTVR, a baseline of 2 MLP machine learning modules utlised within a Python webcam demo to provide an early look at the development. Later a Unity-specific version will be created, that can be integrated into a metaverse or game. 

![alt text](https://github.com/DarrenBellenger2/EmpactVR-MLP/blob/main/paper/PrototypeOne.jpg)

# Facial Emotion Recognition

It is theorised that facial values present in some types of avatars (such as the Unity UMA avatar) resemble Ekman action units. The early results presented show a facial emotion recognition accuracy of up to 92% on one benchmark dataset, with an overall accuracy of 77.2% across a wide range of datasets, demonstrating the early promise of the research.

# Audio Emotion Recognition

The audio emotion recognition part of the research
